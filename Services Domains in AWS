 Different services in AWS:
  1) compute
  2) storage
  3) database
  4) security
  5) management
  6) customer engagement
  7) App integration

1) Compute Domain:
    AWS EC2, Elastic Beanstalk, AWS Lambda, Autoscaling, 
    AWS LoadBalancer, AWS ECR, AWS ECS
    
  1) AWS EC2: Elastic compute cloud is one of the integral parts of the  aws ecosystme. EC2 enables on-demand,
  scalabale computing capacity in the aws coud.
  -Its basically a raw server that is given to you. You can do anything, make a web server or a operating system etc...you can make a ram of your choice, type of os
  like linux or windows etc..
  
  2) Elastic beanstack: you can quiclky deploy and manage applications in the aws cloud without having to learn about the infrastructure
  that runs those applications.In AWS EC2 you can make it to do anything, you can make it a web server, install softwares, make database server. But whereas elastic beanstack 
  has some restrictions, it is lonely a web application server, you cannot install any softwares.
  -AWS ec2 is a infrastructure as a service whereas elastic beanstack is a platform as a service. Everything is pre configured in elastic beanstack.
  -it is used to quickly deploy web applications.
  - It will host an application but only you can only deploy web apps quickly. You can import your websites from local or you can start designing from scratch as well.
  But in EC2 you can do anything.
  
  3) AWS Lambda: It is a compute service that lets you run code without provisioning or managing servers.
    -It is a advanced version of EC2. you can deploy an application in it but you cannot host an application in it.
    -It is used for processing the backend code.
    - It will not host an applicatoin
  
  4) AWS Elastic LoadBalancer: It distributes incoming application or network traffic accross multiple targets such as amazon EC2 instances, containers, 
  and all IP addresses in multiple availability Zones.
  -Its basically keeps track of the activity of the severs. If a server is unhealthy it stops transfering server traffic to that server and send to other healthy server
  until it cools down. This is the main job of the Loadbalancer.
  -For eg: If there are three servers in which the trafiic is distributed if all the three severs are healthy and working fine the loadbalancer will distributes
  the traffic equally among all the three services. But if there is any overload in one of the server the loabalancer will stops sending the traffic to that server 
  and diverts it to the other two server until it cools downs.
  -If we wonder how can we decide how how much traffic is coming to deploy accordingly to servers, then the next service will comes into play which is AWS auto scaling
  
  5) AWS Auto Scaling: Monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possoble cost.
  -So basically it mainatins the track of how much traffic is incoming and how to handle them by distributing the load to different instances.
  -If the incoming traffic is going beyond 80% then the auto scaling automatically deploys additional instances to takecare of the extra load. At the same time if 
  the incoming traffic is below 40%, it decreases the instances and allot them to other resources or keep them in idle mode.
  
  6) AWS Elastic container registry(ECR): It is a fully managed docker container registry that makes it easy for developers to store, manage and deploy docker 
  container images.
  -Basically a docker is a tool which stores many conatiners at one place, where each container is a virtual machine with separate operating system with min size
  of 40MB. So to store these containers ECr sevres as a docker conatiner which hold all these containers at one place. Suppose if there is a windoes operating system
  with 40mb in size conatiner this will be stored in ECR. But if you want to run this conatiner there is another service called ECS( Amazon Elastic service).
  
  7) Amazon ECS(amazon elastic container service): It is highly scalable,high performance orchestartion service that supports docker containers and allows you to 
   easily run and sacle containerized applications on AWS.
   -It basically runs these conatiners whenever necassary and if there is anything wrong with this container it relaunches it again.
   
 2) AWS Storage Domain:
    1) Amazon S3
    2) Amazon Glacier
    3) Amazon Elastic file system
    4) AWS Storage Gateway
    
  1)Amazon S3(Amazon simple Storage Service): It is an object storage service that offers industry-level scalability, security and performance.
      User--> File uploaded or dowmload--> Internet--> Amazon S3 bucket
    Storage domain typically contains the binary file slike images, music files, videos and others. These will be stored in these service. Like, In a web page
    the images, gifs and videos should be loaded when the website opens so all these files are retrieved from here.
    
   2) AWS glacier: It is a secure, durable and extremely low cost cloud storage service for data archiving and long term-back up
        server--> backup-->amazon s3 glacier.
     Glacier is strictly a back up service and low cost. It takes more time to retrive the files from here.
   
   3) Amazon Elastic file System(EFS): It provides a simple, scalable, elastic filesystem for linux based workloads for use with AWS cloud services and on-premises
       resources
       - The best advantage of EFS is, It can be mounted on your server or computer and can be avaialble for all other servers which it is mutually shared.
          When you create a windows server EFS will create a volume for itself and it can be shared with other netwroks if necessary. It acts as a a shared drive
          for 10's and 1000's of computers which is having the same volume. inside them and that same volume would be EFS. The changes in one system will be changed in other systems 
          as well. Can be mounted in both windows and Linux as well.
          
   4) AWS Storage Gateway: It connects an on-premise software appliance with cloud based storage to provide seamless integration with data security features between
   your on-premises IT environment and the AWS storage infrastructure.
   - On premises Application ---> AWS Storage Gateway ---> AWS cloud Infrastructure.


3) AWS Database Domain:
      1) Amazon RDS
      2) DynamoDB
      3) Amazon RedShift
      4) Elasticache
    
    1) Amazon RDS(Amazon Relational Database Service):
        It makes it easy to set up, Operate and scale a relational database in the cloud.
        --It is a service under where we can lauch many services like amazon aurora, mariaDb, mysql, microsoft server etc... But RDS basically manages these databases.
       
    2) Amazon DynamoDB: It is a key-value and document database that delivers single-diit millisecond performances at any scale.
        --It is a NO SQL database, No SQL means whenever you collect unstructured data like the data which does not have any format like sqlm then DynamoDB comes
        into play which manages these kinds of data.
    
    3) Amazon RedShift: It is a fast, scalable data warehouse that makes it simple and cost effective to analyse all your data accorss your data 
    warehouse and data lake.
    --It is a warehouse where multiple database engines are stored and manages to gives output whenever necessary.
      -- Input--> All data is stored in the S3 bucket--> Redshift--> Output.
      
   4) Amazon Elastic cache: It can be used to seamlessly deploy, run and scale popular open source compatible in-memory data stores.
     It is a layer between client and the server, whenever a client is asking for some data, when he writes the query and submits it, the elastic cache process it and 
     gives the output. when the same query is asked again multiple times, then elastic cache stores these frequemtly asked queries and gives the output instantly without
     wasting time by running the query again. It saves the overhead time and increases performance.
     

4) AWS security Domain:
      1) IAM
      2) AWS KMS
      
     1) IAM(dentity and Access Managament): It enables you to manage access to  AWS services and resources securely.
      --It basically restricts multiple user access in a single aws account. Like big companies like netflix, prime has only single account
      where it has multiple user accounts inside for different kinds accesses. So, It restricts this access from one user to other and secures the service in a single\
      umbrella.
      
     2) KMS(AWS Key management service): It makes it easy for you to create and manage keys and control the use of encryption acrss a wide range of aws services
     and in your applications.
     --we are using key pairs to create a service in ec2, so these key pairs functionality is handled by this KMS service. If we want to create a new key pair,
     we just have to  create a new one.
     -- Plaintext--> KMS--> Cipher Text
                     (Data key)
                     
5) AWS Services Management:
    1)AWS Cloudformation
    2)AWS OpsWorks
    3) AWS CloudTrail
    4) Cloud Watch
    
     1) AWS Cloudformation: It provides a common language for you to describe and provision all the infrastructure resources in your cloud environment.
     
     2) AWS OpsWorks: It is a configuration service that provides managed instances of chef and puppet. Chef and puppet are automation platfroms.
     
     3) AWS CloudTrail: It is a service that enables governance, compliance, operational auditingand risk auditing of your AWS account. With cloudtrail, you can log, continoulsy
     monitor and retain account activity related to actions accross your AWS infrastruture.
     --It is basically stores the log data of whats happening in your aws account and keeps track of everything, every llogin details, the changes made by the users
     and every digital footprint that is happening in your aws account. we just need to connect to aws cloudtrail to access this feature. And this log data can be \
     further used to analyse further for future developments.
     track user activity and API usage:--> furure usage
          capture-->act--> store--> review
     
     4) Cloud watch: It ia monitoring and management service built for developers system operators, site reliability engineers(SRE), and IT managers.
        -- It sets an alarm if there is any action is needed like it will send an email about any event and lets you know.
        
6) AWS customer engagement domain:
        1) Amazon connect
        2) Simple email service
        
        1) Amazon connect: It is a ready customer contact center, which can help you setup your IVR calls to agents in the shortest time possible.
        2) Amazon Simple email service(Amazon SES): It is a cloud based- email sending service designed to help digital marketer and application developers send marketin, notofication, 
        and transactional emails.
        
 7) AWS App Integration:
 
        1) Simplw notification service
        2) Simple queue service
        
        1)Amazon Simple notification service(SNS): It is a highly available, durable, secure, fully managed pub/sub messaging service
        that enables you to decouple microservices, distributed systmes and serverless applications.
        --It filters the notifications recieved from the publishers and send it to the relevant services like aws lambda or aws sos etc..
        
        2)Amazon Simple queue service(SQS): It is fully managed message queuing service that enables you to decouple and scale microservices
        distributed systmes and serverless applications.
        
8) AWS pricing:
1) Pay as you go: It is widely used model. Whatever amount of time you will be using the server for, you will be charged for that 
and the rest is given to you.
2) save when you reserve: Giving the time of usage like 3 years of continous usage. This will enables aws to offer the discount.
3) pay less by using more: The more you will be using instance the less the pricing will comes down.



---------------------------------------------------------------------------------------------------------------------------------------------------------------
ESSENTIALS OF CLOUD COMPUTING:

CLOUD MODELS:

Public, Private, Community and hybrid cloud models.
1) Public cloud model: 
   --Public services are owned by the cloud service providers and services are delivered over internet.
   -- Unlimited resources, multi tenant systems and resilient towards any failures(can be repaired in no time). Maintainance can be taken care by the service 
   providers.
    --Best suited for: Variable workload, Test & developement
    --Advantages: Lowest TCO(Total cost of ownership), Faster deployment and rapid elasticity.
    --Challenges: Data security and privacy.
    -- when you need to conduct a survey in several geographic locations, public model is best, after completing the surveys
    and collecting the required data you can simply terminate the instances on those locations if we are not using anymore.
    
2) Private cloud: It is operated soley for a specific organization and the infrastructure can be build on premise or online.
    --Single tenant, data location can be anywhere. Compliance and regulatory requirements.
    --Best suited for: sensitive data and legal compliance.
    --Advantages: Security and control. Optimized performance.
    --Challenges: High cost of ownership and skillset is required.
    egL Givernment agencies uses this private cloud and it is highly costly.

3) community cloud model: It will be shared by serveral organizations for a common purpose and the services are offered through internet.
    --Infrastructure is shared, multi tenant and community driven governanance.
    --Best suited for: Collaboration of universities, groups of hospitals.
    --Advantages: Lowest TCO and rapid Elasticity.
    --Complex IT governance and skillset.

4) Hybrid model: It is combination of one or two cloud models based on the requirement. Eg: private and public or public and community.
    --benefits of both private and public deployment models.
    --Data exchanges between public and private clouds.
    --control over sensitive assets, flexible.
    --Cloud burst is possible in times of high traffic, it can easily goes from public to private model based on the occassions.
    --Best suited for: Cloud bursting, on demand access and sensitive data
    --Advantages: Lowest TCO, high performance, Rapid elasticity and highly customizable.
    --Challenges: Portbilty, migration and integration


CLOUD SERVICE MODELS:
  1)Software as a service(SaaS):
    --Readily available software application that are already designed and programmed
    --Accessed through web browser
    --No visibilty over backend.
    --Billed based on subscription
    Eg: Microsoft office 365, google apps, microsoft teams and slack channel.
  2) platform as a Service(PaaS):
      --Provides platforms for developing and hosting apps on the Cloud service providers' infrastructure, reducing infrastructure management effort 
      --Runtime, platfrom and tools are set up by the creator
      --No management effort.
      --Less control over infrastructure since the service provider mostly controls it.
      --Control over the application code we desgin and program.
      Eg: App cloud from salesforce.com, google App Engine(GAE) from google cloud platform, openshift from red hat and oracle cloud platform from oracle
  3)Infrastructure as a service(IaaS):
      --Compute, storage and network resources.
      --Control over the OS and cloud storage devices.
      --configure development environment remotely.
      Eg: Elastic compute cloud(EC2) from Amazon web services
          virtual machines from azure
          google compute engine(GCE) from google cloud platform.
         
         
 SERVERLESS SERVICE:
 
    --It allows you to focus on only business productive functionalities rather than the IT infrastructure and management.
    --Capacity provisioning, patching operating systems and other functions are managed by the service providers
    --Reduces Total cost of ownership.
    --Highly available and fault tolerant
    --Also known as function as a service.
    Eg: Consider a weather update in web application, when a user clciks on the weather update, the serverless function invoked and retrieves the data and
    send the response.
    This results in we only get charged for the event of execution only not the times taken to process the event and time taken to get the response from the server.
    --Cost effective option.
    Serverless Service(FaaS)                                               |         Platfom as a service(PaaS)
    -Scales automatically based on the traffic based on number of requests |      Scales automatically by adding adding additional instances running
    -runs for a short tenure, only when invoked                            |       Typically runa 24*7
    -Less control over deployment environment                              |    developers has More control over deployment environment
    -precise pricing, You will pay per use only.                           |  Cost may incur for few instance hours with less utilization.
    
MAJOR CLOUD SERVICE PROVIDERS: 1) AWS 2) AZURE 3) Google cloud

VIRTUALIZATION:
  Instead of creating phyisical machines for business, virtual machines helps to readily available and takes less TCO.
  -- Isolates workloads hosted on physical infrastructure.
  -- Virtual machines are created from the physical infrastructure.
  --Hypervisor manages resources for virtual machines.
  -- Each VM is an isolated environment.
  Two types: 
      1) bare metal virtualization. does not contain any OS on the host. eg: vmware etc..
      2) Hosted virtualixation: contains an OS to run and maintain the administrative functions. eg: orcale, vmbox etcc/
      
      Important questions:
      You are an architect for an enterprise that aims to develop online games with low latency and high performance. 
      which of the following approaches would you choose to deploy the game?
      high performance. Which of the following approaches would you choose to deploy the game?
      Ans: Leverage bare metal or Type 1 hypervisor as it delivers at low latency. TYPE1 or bare metal.

CONTAINERS: 
    Why containerization is performed?
    With these cloud technologies there is a need for the applications to deploy at anywhere anytime. This made the containerization comes into play
    --Need to deploy on multiple IT environments. eg: apache,php,rhel ==> container
    -- Makes the need of responsive applications with low cost
    -- Improve resource utilization
    --Architecting suitable apps
    
    What are containers:
      --It offers light weight runtime environment for app deplayment.
      --Bundles all the required dependencies. 
          App1 -- App2 -- App3-->Docker engine-->host operating system-->host hardware
     Benefits of containers:
        --Rapid scalability
        --Uses less resources
        --increased portability
        --greater efficiency.
        
      Difference between Containers and virtual machines:
        Containers:                                               Virtual machines:
        Boots in milli seconds                                    Boots in minutes
        Requires less resources such as memory, storage etc...    Needs more resources
        Higher isolatoin level                                    Lower isolation level
        Highly portable                                           Highly portable
        Shares OS libraries and kernal                            Doesn't shares OS with the host machine
        No hypervisor required                                    Requires a hypervisor
        
    Containerization using docker:
        --Open source docker engine
        --provides both linux and windows based containers
        --Containerized applications run anywhere consistently
        --Docker CLI, API
        --you can build custom docker container using docker image registry
 
 CLOUD NATIVE APPLICATION DEVELOPMENT:
      1) Microservices: This approach integrates the scalable nature. If one is failed other instances will helps the job done.
          Advantages: Independent, High fault tolerance and enables continous delivery, supported well by containers and scalabilty and reusability
      2) Devops: It is collection of tools and practices that enables high velocity development., Agile in development.
      3) CI/CD(continous integration/continous delivery): 
          --Automates software release process
          --Code chages trigger release pipeline automatically
          --allows numerous deployements in a short span
          Any changes made in the source code will automatically trigger the other stages and deploy the new product without manullay changing the other stages.
      4) containers:
           --Containers support cloud native development
           --container is the basic unit of compute capacity
           --faster start up time and rapid scaling.
           
      CLOUD MIGRATION:
      CLOUD SECURITY:
            Security Threats in cloud:
              unauthorized access, data at rest: Data at one place is vulnerable for attacks. Data in transit: Data traffic in the cloud might be interupted by other
              networks and leakge of data is possible.
            Corporate Datacenter security                                      | Cloud security
            Complete control over the infrastructure protection                | Physical security of the infrastructure is managed by cloud service provider
            Higher total cost of ownership                                     | Lower TCO
            API's to be created to mitigate critical scurity threats           | Provides secure API's to work with cloud resources
            Continous resources monitoring is to be performed                  | CSP's monitor physical hardware, you need to monitor access to your assets
        Cloud vendor is responsible for security of the cloud:
            Software security, network security and gloabl infrastructure security
        Customer is responsible for security IN the cloud:
            Firewall configuration, data security at rest and in transit and also access control.
            Cloud security features:
              --Organization policies
              --access control
              --encryption
              --compliance, monitoring and auditing
            Best practices:
            --Understand the shared responsibility model- most of the acses the security breaches happens due to the indequate understanding of the responsibilty model.
            --Always encrypt your data at transit or rest.
            --Use github secrets or Aws msecrets manager etc..
            --Do not leave out security credentials and tokens.
            Mitigate unauthorized access.
            
 -------------------------------------------------------------------------------------------------------------------------------------------------------------------
 
 AWS SERVICES OVERVIEW:
 
  AWS COMPUTE SERVICES:
   1) Amazon EC2(Elastic Compute cloud): Elastic compute cloud is one of the integral parts of the  aws ecosystme. EC2 enables on-demand,
       scalabale computing capacity in the aws coud.
       -Its basically a raw server that is given to you. You can do anything, make a web server or a operating system etc...you can make a ram of your choice, type of os
       like linux or windows etc..
       --Provides scalable computing
       --Launch as many or as few virtual servers as you need
       --configure security and networking and manage storage.
     Amazon EC2 Features:
   Amazon Machine image: Its a template that defines OS for the serves. This will helps you launch new servers thats already have custom applications and start running
       them instantly without any delay.
   Instance Types: This configures the ram, performance and storage of the EC2 instance
   Key pairs: Use a private key instead of password. We need to provide the private key to login to your server, Once lost we cant access the server. It can never be
       recovered. It must be protected carefully.
   Instance store volumes: For temporarily store the data and will be gone after logged out of the server.
   Amazon EBS volumes: These are  used to quickly access the data since these are fast devices.
   Regions and availability zones: Amazon Ec2 services can be launched anywhere anytime if we had a system and active internet connection.
   Security groups: They allow you to provide ports and network traffic.
   Elastic IP addresses: This feature provides static and dynamic ip addresses for dynamic cloud computing.
   Tags: tags helps you to classify the resources.
   Virtual private cloud: Helps you to decide which amazon vpc you want to launch.
   AWS EC2 pricing models:
    1) On demand instances: The payment will done in hourly or seconds type. No need to commit for the long term and also pay for what you use.
      --No Up-front payment or long term commitment.
      --Worloads that cannot be interupted.
      --Developed or tested on amazon EC2 for the first time.
      --a1.medium costs 0.0255$ per hour.
      --a1.large 0.051$ per hour
    2) Spot Instances: Needs in such case of high computing in short time. Gets upto 90% discount.
       --Uses for Flexible start and end times, Have committed term usage 
       --Very low compute prices.
       --Urgent computing needs
       --a1.medium costs 0.0049$ per hour a1.large $0.0098 per hour. 
    3) Reserved instancs: Can be used for a long time committment like 1 year or more.
        --Applications with steady state usage
        --Applications that need reserved capacity
        --Avaialable for 1 or 3year term
        a1.medium - $0.015 per hour and a1.large $0.030 per hour.
        EBS: Amazon Elastic block store: Provides elastic block storage for the EC2.
           --EBS has 4 different types of volumes types to meet the anytype of workloads.
           SSD: Delivers low latency and highest iops
           HDD: Provides highest throughput.
           
 AWS Elastic Beanstalk:  you can quiclky deploy and manage applications in the aws cloud without having to learn about the infrastructure
  that runs those applications.In AWS EC2 you can make it to do anything, you can make it a web server, install softwares, make database server. 
  But whereas elastic beanstack has some restrictions, it is lonely a web application server, you cannot install any softwares.
  -AWS ec2 is a infrastructure as a service whereas elastic beanstack is a platform as a service. Everything is pre configured in elastic beanstack.
  -it is used to quickly deploy web applications.
  - It will host an application but only you can only deploy web apps quickly. You can import your websites from local or you can start designing from scratch as 
  well.But in EC2 you can do anything.
  
  Why we need Beanstalk: For ex, Jack is a developer and he wants to focus on writing a code and does not want to divert his attention to systme administration.
    like spending lot of time confirguring managaing server loadbalancers and data networking instead of coding. To focus on only on coding and let others take 
    care of the system administration tasks.
      Elastic beanstalk helps this way, It is easy to use and deploy and scale applicatons.
      eg: Java,.net,node.js,python, ruby and docker etc.. can run easily.
      --You just need to write the code and upload it. 
      
AWS Lambda: 
    It is a compute service that lets you run code without provisioning or managing servers.
    -It is a advanced version of EC2. you can deploy an application in it but you cannot host an application in it.
    -It is used for processing the backend code.
    - It will not host an application.
    --Why Lambda?
      When you need lots of fucntionalities like uploading an image, in app purhcases and website clicks. All these funcitons will happen only by running specific 
      codes designed in back end. But managing back end codes, manage operating systems, apply security patchs and scale the servers when there is huge traffic 
      is not possible practically, this is where lambda comes into play.
      AWS Lambda is a compute service that runs your back end code and responds to events such as objects uploads to amazon s3 bucket and updates amazon dynamodb table
      or manages data in kinesis stream or controls in app activity. Once you upload the code in the lambda it controls all these additional functionaltites like
      manages OS, patches security alerts, responds to the events, scaling and administration  and sends the real time matrics and logs to the amazon cloud watch.
      all we need to do write the code. We simply charged for the usage we did in the server.
       
      

 AWS Storage Domain:
    1) Amazon S3
    2) Amazon Glacier
    3) Amazon Elastic file system
    4) AWS Storage Gateway
    1)Amazon S3(Amazon simple Storage Service): It is an object storage service that offers industry-level scalability, security and performance.
      User--> File uploaded or dowmload--> Internet--> Amazon S3 bucket
    Storage domain typically contains the binary file like images, music files, videos and others. These will be stored in these service. Like, In a web page
    the images, gifs and videos should be loaded when the website opens so all these files are retrieved from here.
    --provides easy to use management features.
    Features of S3: 
      --store as much as you need
      --Keep multiple verions of objects
      --Host static websites.
      --replication of data if data is lost and need to back up.
      --pay as you use.
      --has different storage class.
    S3 Use cases:
    --Back up and restore
    --disaster recovery
    -- Archive data--> S3 glacier and deep arcive are cost effectitve for archiving the data
    --For big data analytics.
    --Hybrid cloud storage
    --cloud native applications.
  1) Amazon S3 Storage Classes:
     There are different types of storage classes in S3:
     1) S3 Standard: Used when data objects are most frequeltnly accessed.
     2) S3 Standard IA: Used when data objects are less frequently accessed.
     3) S3 Intelligent Tiering: For changing the data class from S3 standard and S3 standard IA if necessary can be done in this class. 
     4) S3 One Zone-IA: Its present in one available zone only and the price is 20% less than others.
     5) S3 Glacier: durable of 99.999% of objects across multiple AZ's, used for back up files
     6) S3 Glacier Deep Archive:  This is for retaining the data for very long time, Its rarely accessed. Financial services,, health care
         weather etcc.. data can be stored here.
   2) AWS glacier: It is a secure, durable and extremely low cost cloud storage service for data archiving and long term-back up
        server--> backup-->amazon s3 glacier.
     Glacier is strictly a back up service and low cost. It takes more time to retrive the files from here.
   
   3) Amazon Elastic file System(EFS): It provides a simple, scalable, elastic filesystem for linux based workloads for use with AWS cloud services and on-premises
       resources
       - The best advantage of EFS is, It can be mounted on your server or computer and can be avaialble for all other servers which it is mutually shared.
          When you create a windows server EFS will create a volume for itself and it can be shared with other netwroks if necessary. It acts as a a shared drive
          for 10's and 1000's of computers which is having the same volume. inside them and that same volume would be EFS. The changes in one system will be changed in other systems 
          as well. Can be mounted in both windows and Linux as well.
         
          
   4) AWS Storage Gateway: It connects an on-premise software appliance with cloud based storage to provide seamless integration with data security features between
   your on-premises IT environment and the AWS storage infrastructure.
   - On premises Application ---> AWS Storage Gateway ---> AWS cloud Infrastructure.

 2) AWS glacier: It is a secure, durable and extremely low cost cloud storage service for data archiving and long term-back up
        server--> backup-->amazon s3 glacier.
     Glacier is strictly a back up service and low cost. It takes more time to retrive the files from here.
   
   3) Amazon Elastic file System(EFS): It provides a simple, scalable, elastic filesystem for linux based workloads for use with AWS cloud services and on-premises
       resources
       - The best advantage of EFS is, It can be mounted on your server or computer and can be avaialble for all other servers which it is mutually shared.
          When you create a windows server EFS will create a volume for itself and it can be shared with other netwroks if necessary. It acts as a a shared drive
          for 10's and 1000's of computers which is having the same volume. inside them and that same volume would be EFS. The changes in one system will be changed in other systems 
          as well. Can be mounted in both windows and Linux as well.
          
   4) AWS Storage Gateway: It connects an on-premise software appliance with cloud based storage to provide seamless integration with data security features between
   your on-premises IT environment and the AWS storage infrastructure.
   - On premises Application ---> AWS Storage Gateway ---> AWS cloud Infrastructure.
   
AWS Networking services:
  Amazon VPC(Virtul private cloud): It lets you set up the aws cloud in a logically isolated section
     --Have complete control over your virtual networking
     --Easily customize the network configuration.
     --use multiple layers of security.
   Amazon VPC componenets:
     --Subnet: a range of IP addresses is a subnet, while launching the resource we can specify the subnet. A public subnet can accessible from the internt and the
     private subnet does not accessible from the internet.
     --route table: A set of rules that guides network traffic in amazon vpc. A subnet is particularly associated with a route table otherwise it is
       implicitly associated with a route table. Every route in a route table has specific range of IP adrressses where you want the traffic to be directed i.e the
       destination in the gateway.
     --Internet gateway: Its a virtual device which connects the vpc and internet. 
     --NAT gateway: you can use NAT gateway to enable instances in private subnet connect to the internet.
     --VPC end points: Using vpc end point we can create a private connection between vpc and suuported aws services like s3, ec2 etc..
     --VPC peering: this feature for enabling communcation between two vpc's over the private IP addresses. VPC peering is possible between the vpc in different regions
       in different aws accounts.
   Introduction to amazon cloudfront:
      It is a fast, highly secure and programmable content delivery network service.
      --You can deliver content to millions of users ata a itme and it can be static or dynamic content.
      --You can securely deliver the content to your customer securely, Cloudfront provides free certificate  from amazon certificate manager 
      --cloudfront also sheilds the network 4,5 level security threats
      --cost effective
   Introduction to Amazon route 53:
        Usually DNS(Domain name system) works as a language of numeric IP adresses from one computer to another computer by reading different site anme like www.google.com, facebook.com etcc....
        Route 53 is a DNS service that gives developers and adminsitrators helps a cost effective way to route end users to internet applications.
          --It integrates seamlessly with other aws services like ec2, s3 and cloudfornt etcc to work together effectively for better outcome
          --If one end user is failed it will reroutes to other end point as an alternative
          --It gives 100% availability, integrates easily with other aws services and helps improves reliabilty with traffic flow.
          
AMAZON DATABASE SERVICES:
      1) Amazon RDS
      2) DynamoDB
      3) Amazon RedShift
      4) Elasticache
    
    1) Amazon RDS(Amazon Relational Database Service):
        It makes it easy to set up, Operate and scale a relational database in the cloud.
        --It is a service under where we can lauch many services like amazon aurora, mariaDb, mysql, microsoft server etc... But RDS basically manages these
        databases.
        Why RDS: To maintain databse services one has to overcome the challeges like
        --patching,installation, hardware upgrades, security, scaling, backups, performance tuning, monitoring etc...
     Amazon RDS can handle all these challenges at one place.
      --Can create and scale relational database easi;y, automates time consuming administration and lets developers soley focus on the 
      application management instead of worrying about the infrastructure management.
      -- You can choose any of the six amazon rds types for your business needs:
      postgreSQL, Amazon aurora, MYSQL, Oracle, MariaDB and SQL server.
    Benefits of RDS:
      --easy to administer
      --highly scalable, available and durable
      --fast, secure and cost effective.
      
        
       
    2) Amazon DynamoDB: It is a key-value and document database that delivers  millisecond performances at any scale.
        --It is a NO SQL database, No SQL means whenever you collect unstructured data like the data which does not have any format like sqlm then DynamoDB comes
        into play which manages these kinds of data.
        With all the new database engines and new challenges to store the data sequentially, Amazon dynamo DB helps to scale huge requests, handles lots of traffic at a time
        During amazon sales day, great indian festival dynamo DB helps to scale the usage of the database services.
        --It is safe and secure way to handle database services and can handle one trillion requests per day.
        
    3) Amazon RedShift: It is a fast, scalable data warehouse that makes it simple and cost effective to analyse all your data accorss your data 
    warehouse and data lake.
    --It is a warehouse where multiple database engines are stored and manages to gives output whenever necessary.
      -- Input--> All data is stored in the S3 bucket--> Redshift--> Output.
      --processing and retrieving data from the data warehouses without worrying about the datawarehouse.
      -- But maintaining data warehouses is very costly and challenging.
      --So Amazon redshift overcomes all these challenges
      --Amazon redshift serverless allows you to start, run and get data analystics in fast way possible without thinking about the warehouse management.
      -- It automatically scales and provisions the warehouse management where you only need to focus on insights of the data 
      --It can allow share and query live data across organizatoins, accounts and even regions.
      
      
   4) Amazon Elastic cache: It can be used to seamlessly deploy, run and scale popular open source compatible in-memory data stores.
     It is a layer between client and the server, whenever a client is asking for some data, when he writes the query and submits it, the elastic cache process it and 
     gives the output. when the same query is asked again multiple times, then elastic cache stores these frequemtly asked queries and gives the output instantly without
     wasting time by running the query again. It saves the overhead time and increases performance.
     
     
AMAZON DEVELOPER TOOLS:
  1) Introduction to AWS Cloud9: 
  2) Introduction to AWS CodeCommit:
      As your project and developement team grows larger finding a  highly available and secure ways to store version code becomes a challenging and costly, 
      Its easy if we dont need to bother about managing instead focusing on code development.codecommit will solve this problem by giving all these functions in one platform.
     Code Commit will helps in,
     -- Secure, highly scalable and private git repositories.
     --No hardware to manage and software to patch up. It is highly scalable, available and accessible.
     --with code commit we can store binaries, images, and libraries.
     --AWS code commit auttomatically encrypts your files in transit and at rest too. We dont need to worry about security.
     --It integrated with identity and access management, this lets you control who has access to which application. One can chage the access from time based, 
     region based etc..
   3) Introduction to AWS CodeBuild:
     With able to keep up with the competetion software developers need to deliver code changes to allow them in different set of applications and devices.
     --This includes rapidly build and test code. But when there is athreshold developers need to wait for the long waiting time to build the code.
     --AWS code build is fully managed continous integration service that can compiles source code, runs tests and produces build artifects. your team just needs to 
     focus on commit code changes and code build will handles the code building, testing and compiling concurrently without any delay.
     --You pay for only the build minutes you use. You can bring build your own environment or you can use already pre compiled environments.
   4) Introdcution to AWS CodeDeploy:
      Every developer always in a loop of developing the code, tetsing it and deploying it to the production. But Manually deploying the server will takes long time
      results in errors and downtime. It causes complex and hard to maintain
      --It would be nice if we automates the deployment of your application and you can rapidly deploy into production.
      --AWS Codedeploy, A service that can coordinates application deployment and updates across amazon EC2 instances of any size.
      --It automates your application deployment getting rid of the the need to do things manually.
   5) Introduction to AWS CodePipeline:
      In today's booming IT industry, developing applications rapidly and deploying them is the core success fo the many software companies.
      Using traditional software process delivering and testing applications fastly is difficlut and error prone where Code updates are manullay developed, tested 
      and deployed. What is all the software process is automated and allows you to test and release the code more frequently.
      AWS CodePipeline:
      --It allows you automatically test, deploy the code automatically without any delay.
      --There will graphical funcitonality in the service where you can able to manage the code deployment's entire process.
      --It works as pipeline where in each segment, it will automatically do its job like testing, building and deploying etc..
      --Once the pipeline is ready, It will automatically build, test and deploys the code.
      --It sets manual approval at each stage of the pipeline whether we need to approve or deny the code feature if it does not meet the standards.
      --Code pipeline also stops the pipeline whenever an action fails to comply its rules such as unit test failure.
      --It checks the minor and major bugs rapidly and deploy the code more frequently. this allows us to focus more on core product.
      --Code pipeline is extensible and works with variety of source code, build, testing and deployment tools from AWS or third parties. It enables us to use 
      different services at each stage of pipeline.
 
 Introduction to AWS Monitoring and auditing Services:
   1) AWS Config: 
     -- AWS resources configuration evaluation
     --Review relationships
     --Detailed configuration histories
     --Compliance check.
   2) AWS Trusted advisor
     --provision resources following AWS best practices
     -- Covers-cost optimization, performance, security, fault tolerance and service limits
     --Recommendations provided.
   3) Inspector: 
     --Vulnerabilities check
     --Detialed assessment reports
     -- check for vulnerabilties on EC2 instances.
   3) Flow logs:
     --IP traffic capturing
     --Charges to deliver logs to  s3 apply
     --VPC, a sbunet or a network intefere.
   4) AWS cloud trail:
     --AWS account auditing and compliance check
     --tracking and troubleshooting
   5) AWS cloudwatch:
     --monitoring and observability service
     --logs, metrics and events
     --set alarms and automate actions.
     
     
AWS Identity services:
   When using a sinble aws account from  multiple users will concurr some problems with eprmissions to the data, unauthorized access. AWS IAM will helps in 
   maintain these multiple logins and administartes it.
   --IAM is global service
   --create and manage IAM users and groups
   --use permissions to define authorization
   --no additional charge
   Use cases:
    access control: 
      --Control accss to aws services and resources
      --condition based controlling
     Multifactor authentication:
       --No extra cost
       --Users should provide physical possession of MFA enablesmobile device and a hardware MFA token.
     Analyze access
       --Analyze access across your aws enivironment
       --principle of leasr privilege
     Integrate with corporate directory
       --Federated access
       --use SAML 2.0 or use one of AWS's federation samples
       
     There are three types of identities in IAM: users, groups and roles
       --Users: represents person that uses IAM account
                Grant permissions by adding an IAM account to a group. permission of an IAM user can be cloned to another user when nw user is addedd.
       --Groups: Collection of IAM users is called groups
                 Use groups to specify permissions for a collection of users, If you want to restrict some users for the specific tasks create a group with those 
                 permissions and add the users to that group. Eg: If we want some users should have access to admin then create a group with admin permissions
                 and add users to that admin group.
       --Roles:  contains policies, but unlike users it does not need any credentials associated with it.
   Introduction to amazon congnito:
      Imagine you are playing a game in your mobile device and you made it to 50 level in that game and suddenly you switched to tablet device with same login credentials 
      but you noticed that the game reset to 1st level. This is because the failure to integrate  accounts with multiple  devices. this needs a backend code to be 
      developed by the creator additionally. Instead AWS amazon cognito helps in maintain this user data in multiple platforms withour losing any saved data.
      You can login as a guest and startt doing your work later you can add your account and still it will be reflected in all of your devices.
      You just ned to synch to amazon congnito.
      
    Introduction to Amazon SSO(single sign on):
      If you have many users with diferent types of softwares and app login credentials.  All this data must be stored somewhere and have to maintain these login 
      attempts separately which might take an extra cost, infrastructure and expertise. Amazon Single sigh on will helps in this sector to let users to access all
       of their business accounts and softwares in one go after login into their respective aws sso portal.
       
===============================================================================================================================================================

AWS- COMPUTE AND NETWROK SERVICES:

RUNNING WORKLOADS ON EC2:
   Amazon machine Image(AMI): When EC2 instance is launched, An AMI is the first thing that must  be selected, 
      --It is a read only file systme
      --It includes OS-Windows or linux or other softwares before launching an instance
      --AMI ID is specific to a region.
  *EC2 Instance types: 5 types
     1)General purposes: Provides balance in between compute, memory and networking. eg: t2.micr0,m5.large etc...
     2) compute optimized: The instances under this category requires high performance processors. Suited for workloads like batch processing, scientific modelling 
        and compute intensive applications. eg: c6.large, c5.large etc..
     3)memory optmized: These are instances deliver to provide fast performance like analytics and in-memory database. eg: r5.large, x1.16xlarge etcc 
     4)accelerated computing instance: These type of instances provides hardware accelerations like GPU accelerated hardware. suitable for workloads 
     such as deep learning, machine learning , high floating-point workloads and seismic analytics. eg: p3.2xlarge, p2.xlarge etc..
     5)storage optimized: These type of instances are suited for workloads which require high sequential read and write access. such as NOSql database, 
     data warehousing and high performance workloads. eg: i3.large, d2.xlarge etc...
         With AWS EC2 we will get large variety of instances ranging from small type of configuration like 1vCPU 500MB to high configurations like 448 CPU 
         cores with 28TB memory.
   *AWS Instance Launch Types:
     1) On-demand Instances: In this launch type instance, you will pay for duation you have used and there is no long term commitments. They are charged
     for only "running" state instaces. suited for short term and irregular workloads.
     2)reserved Instances: These are meant for the predictable and long running workloads which can run more than an year. commitment term- 1 or 3yrs. 
     Upto 72% discount as compared to on-demand launch instance. Three types of payments is possible: No-upfront, partial upfront and all upfront.
       reserved instances has three types again: 
       standard RIs: suited for steady state usage
       Convertible RIs: ability to change instance family/attributes
       Schedules RIs: launch instances in a specific time frame
     3) Spot instances:  In this aws allows unused EC2 instances in an availabilty zone, to be purchased at a price much lower than the on-demand pricing. 
         you can get discount upto 90% as compared to on-demand pricing. Spot instances are meant for various workloads which are stateless, fault tolerant,
         test and development workloads and workloads which are resistance to interuptions.
    * EC2 Instance Tenancy: Based on the tenancy of the underlying hardware where the EC2 instances will be launched, there are 3 different launch types
        1) Shared Tenancy: This is the default tenancy. The underlying physical hardware of the EC2 instances is shared with other AWS customers, but 
            all the instances will run in isolation without any collision. this is budget friendly option.
        2) Dedicated Hosts: Entire physical server dedicated to an aws user, you will get an entire physical server for use to launch EC2 instances.
            Full control over instance placement. full visibilty into the ssockets and physical cores. It is expensive but useful for the companies which 
            follows strict regulatories about their product. It is suited for scenarios when software pr applications running have a complex licensing model
            Like: BOYL(bring your own license.
        3) Dedicated instances: Entire physical server dedicated to an aws user. The underlying physical server is not shared with other aws users but may be
            shared with instances in the same aws account. No visibility over the sockets or physical cores.
    * EC2 Instance Storage:  
       EBS Volumes: EBS volumes provide persistent and durable block level storage for the EC2 instances. Instance storage dependent on instance type and family.
       Instance store volumes: It proviedes temporary block level storage for the instances. Data is lost when the instance is stopped and then started or if 
       it is terminated. Instance storage is dependatn on instance type and family.
   * EC2 Placement groups: By default instances spread across underlying hardware in an AZ.
       --Reduces risk of correlated failure
       -- Custom placement groups can be created based on workload requirements.
       Based on stratagy there are of three types:
        1)Cluster placement strategy: It allows instances to be tightly grouped in an availability zone.
             This enables workloads to achieve low-latency network performance.
        2) Partition strategy: It allows to spread instances acorss logical paritions such that group of instances in one partition do not share the underlying 
              hardware with other groups. This is suited for distributed and replicated workloads such as hadoop, kafka etc..
        3) spread strategy: It strictly places a groupof instances acorss distinct underlying hardware to reduce correlated failures.
    * EC2 security groups:
         when EC2 instances are launched in an availability zone, a virtual firwall called security group, and the loads the instance which controls the inbound
         and the outbound traffic of the EC2 instance using inbound and outbound rules.
         --It is an entity which exist independently from the EC2 host and it is a part of AWS VPC infrastructure. Some key points are
           -In security group the inbound rules control the traffic incoming to the instances and the outbound rules control the traffic outgoing from the instance.
           - One instance can be associated with multiple SGs.
           -If the security group is not specified the instance is launched with default SG.
           -SG rules can be modified at anytime, after instance launch as well.
     
  IMPLEMENTING AUTO SCALING USING EC2:
     During the festive season, there is huge traffic to so many websites and applications to maintain crashes. Manually increasing the workload on the servers and 
     scaling them according to the needs is challenging. So AWS ASG(Auto scaling groups) wil offer auto scaling wheneevr necessary.
     Auto scaling ensures:
      --High availability
      --Improve fault tolerance
      --lower costs
    *EC2 Auto-Scaling Groups - Working Concepts:
    what is auto scaling groups: auto scaling groups are a logical collection of related EC2 instances.
    --minimum size of auto scaling group ensures that the number of EC2 instances in the group never goes below
    that size.
    --Desired capacity of the group can be set, to ensure that the auto scaling group has that many instances at a given point of time.
    --maximum size of the auto scaling group, controls the maximum number of EC2 instances to which the group can scale.
    *Auto scaling benefits:
      --Improved fault tolerance: In EC2 instance, if auto scaling detects unhealthy instance, then it terminates and launches a new one to replace it. In multi -AZ 
      deployment scenario, in case of the failure of an AZ, the scaling can launch instances in another available EC to compensate
      --Higher availablility: EC2 auto scaling always ensures that your application has the right amount of capacity to handle the oncoming traffic demands.
        scale based on traffic.
      --Improved cost management: With auto scaling you dont need to provision capacity before hand, you scale in and scale out based on the requirement, and you pay
      for the capacity you use.
    *Launch configuration: A launch configuration is a set of configuration settings for an instance which the auto scaling group uses to launch EC2 instances
      --In a launch configuration you need to specify information and settings required for launching the instance which include 
         -Amazon machine image(AMI) ID,
         - Instance type
         -Key-pair
         -Security groups
         -Block device mappings
         -EC2 user data if any
         -Other instance configuration parameter's.
       So launch confirguration acts as the pre defined instances launch setting using which auto scaling groups can launch instances automatically based on scaling policies.
       Launch configurations cannot be modifies after the creation and only one launch configuration can be specified for an auto scaling group.
       You can use a launch template for an auto scaling groups as well.
       -- A launch template and launch configuration are similar, as it also specifies configuration information required to launch instances by an auto scaling group.
      Types of scaling:
       1) Manual scaling: As the name suggests you can change the size of an auto scaling group manually either by adjusting the desired capacity or by updating
         instances attached to the auto scaling group.
       2) Dynamic scaling: You can define how to scale the instance capacity in the group in response to changing the demand. you specify scaling policies to scale your 
         auto scaling group in response to demand. 
         example: Scaling policy can be set which automatically scales the instance in the auto scaling group if the average CPU utilization of the instance go beyond
         60%.
       3) Scheduled scaling: You can specify your own scaling schedule. scaling actions for the auto scaling group are automatically performed as a function of time
           and date. Ex: If your application has a predictable traffic increase on a specific day of week, you cna choose to scale out on that specific date and time 
           and then scal eback in after the schedule.
          So, based on your requirement or workload patterns you can choose to perform either, manual scaling or dynamic or scheduled scaling.
          
   MANAGING APPLICATOINS TRAFFIC USING AWS LOADBALANCER:
        **Introduction to AWS Elastix Load Balancing(ELB):
          After implementing auto scaling for the EC2 instances, the solutions architect were facing a new challenge. For example, due to increases in the traffic
          of an online fashion website, the average application hits to their online web store increase to 10000 hits per day. with such increase in traffic 
          customers are facing generla slowness in the application, due tot this the number of EC2 instaces desired to maintain the application traffic had to be
          increased substabially. Their in-house load balances are unable to scale to meet the growing application traffic. They require a solution from AWS which
          will allow them to manage the huge network traffic between the EC2 instances and increase application responsiveness and can scale automatically based on
          the application traffic. so for this scenario AWS offers elastic load balances service which can manage the application traffic between multitple EC2
          instances and scale itself automatically to meet the growing netwrok traffic of the application.
        --AWS Elastic Load Balancer: Elastic load balancer is a scalable solution from AWS which can automatically distribute traffic across multiple targets, 
        such as EC2 instances, containers and lambda fucntions. ELB can handle the varying traffic load of your application either in a single availabilty zone or
        across multiple availability zones in a region. ELB offers features such as high availability, automatic scaling and ronust security measures to make 
        applications fault tolerant. features and benefits of elastci load balancer
        -- High availabiltiy, it can automatically distribute traffic across multiple targets or a group of EC2 instances, spread acorss multiple avaialbility zones
        in a region to make your application highly available to end users.
        -Health checks: ELB offers health checks: It can detect unhealthy targets, and stops sending traffic to the, for consistent user experience. It offers 
        security, as ELB can reside within a VPC and traffic can be controlled using a security group. It provides the feature of integrated certificate 
        management. 
        --SSL/TLS termination: Load balancing can be done either at layer seven or at the application level or at layer 4 for TCP and UDP traffic or at layer of the
        OSI network model. Performance metrics and monitoring can be easily achieved as it is integrated with amazon cloud watch. 
     ** Elasctic Load balancer- Key concepts:
         --Working of elastic load balancer: The load balancer is configured to accept incoming requests by configuring a listener.
         --A listener is the process which keep on checking for incoming connection requests and is configuered with a protocol and port number for connection from
         clients. requests from the listener are forwarded to the resources under the registered targets of the load balancer. The registered targets can be EC2
         instances in single or multiple availability zones, containers, IP addresses or lambda functions. AWS provides 3 types of elastic load balancer:
         1) Application load balancer: It is a layer seven load balancer and is suited for web applications with http and https traffic.
           --It operated at the application layer or the 7th layer of the open systems interconnect(OSI) model. It supports Http type of traffic and is suited for
           web application workloads or containerized workloads. its features include, Path based routing, host based routing, native http/2 support, user authenticaton etcc.
         2) Network Load balancer: It is a layer four load balancer and is used for providing ultra high performance for TCP and UDP traffic.
           --It operates at layer four of the open systems interconnect(OSI) model. It supports various TCP, TLS and UDP traffic. It can handle millions of 
           requests per second, provides ultra low latency and is suited for workloads which require very high network performance. 
         3) Gateway Load Balancer: It is a layer three load balancer. which listens for a IP packets. Classic load balancer is a previuos load balancer for http,
             https and TCP traffic.
             -- It operates at the layer three of the open system interconnect(OSI) model. 
             --This type of load balancer listens for packets across all ports and forwards it to the target that specified in the rule of the listener. This 
             load balancer exchanges application traffic using the GENEVE protocol on port 6081. 
             --It allows you to deploy, scale and manage virtual applicances such as firewalls, intrusion detection and prevention systems. 
         4) Classic load balancer is a previous generation load balancer for http, https and TCP traffic. Application, network and gateway load balancers are 
         newer generations of load balancers offered by AWS and classic load balancer is a previuos generation load balancer. AWS recommends the usage of the 
         newer generation of load balancers. 
         benefits of migrating from classic load balancer:
           --ALB supports various traffic routing policies.
           --ALB supports redirecting of requests from URL's
           --NLB can scale to millions of requests per second.
           --NLB supports assigning of static IP address.
     DEPLOYING WEB APPLICATIONS USING ELASTIC BEAN STALK:
        why do we need elastic beanstalk requirement? due to great global success of the websites, they need to keep designing their web site frequently to 
        handle the bugs and give new features to the customers. Challenges like continous designing and development, and reducing management and time
        to market. They needed a platform that can help them to develop and deploy features to their web application in the least possible time.
     So here comes elastci beanstalk which is a platform as a service(PaaS) to improve team productivity by allowing developers to easily develop and deploy
     fearures to their web application.
     Elastic beanstalk: It is solution which allows for deploying and scaling web application and services written using java, .Net, PHP, etc.. on popular
     servers such as apache, Ngnix, IIS etc..With AWS elastic beanstalk developers can easily upload their application code and service automatically takescare 
     other aspects such as resource provisioning, load balancing,auto scaling and monitoring. 
     Elastic beanstalk features and benefits:
      --Developer productivity- Since beenstalk operates, provisions and manages the infrastructure and the application stack, it frees the developer from having
      to worry about managing and configuring servers, databases, load balancers, networks etcc.
      --Complete resource control: Users have the freedom to choose and select the aws resources such as EC2 instance typ, and allows to explore under the box
      and retain full control over the aws resources powering the application of the user. 
      Monitoring: Elastic beanstalk provides a unified interface for monitoring application health, along with logging and tracing.
      --Scaling: elasctic beanstalk leverages auto scaling and load balancers to automatically scale your application based on the applications need.
      --Deployment options: With beanstalk to deploy your applications you can choose from mulitple deployment policies such as- All at once, rolling and blue/
      green.
      --Complaince: Elastic beanstalk is compliant with various international standards such as ISO, PCI, HIPAA eligility etccc. making it suitable for 
      various types of compliant related workloads and applications. 
   ** Working with elastic  beanstalk is a compute service offered by AWS under platform as a service(PaaS) model.
      --Its supports applications developed on Go, Java, .Net,  node js, php, python and ruby.
      The following workflow illustrates the working of the elastic beanstalk:
      1) create the application
      2) upload a version of the applicaition in the form of an application bundle, for eg a WAR file in the case of a java application.
      3) after that elastcic beanstalk automatically launches the environment such as amazon EC2 resources, auto scaling groups, load balancers etccc. to run the
      uploaded application.
      4) once the application is delployed succefully, it can be easily managed by a unified console, and updates to a version and new revisions can be deployed
      seamlessly.
   ** Elastic beanstalk concepts:
     -- Application : It is defined as a logical collection- of other beanstalk components include, environments, versions and environment configurations.
       Conceptually and application in beanstalk is a folder. 
     -- Application version: Applications version in beanstalk is the name suggests point to an iteration of deployment code for
       web application. The application version are kept in a S3 bucket created by beanstalk, The bucket contains the deployable code such as JAVA WAR file etc
     --Environment:  Environment in beanstalk is the collection of AWS resources which beanstalk provisions to run your application version.resources include
         EC2  instances, auto scaling groups etcc.
     -- Environment tier: While launching an application on beanstalk an environment tier needs to be specified which which determines the type of AWS resources
         to be provisioned. two types: Web environment tier HTTP(S) applicaitons. worker environment tier - backend applications.
     --Environment configuration: It allows you to specify a collection of parameters and settings,- which defines how the provisioned environement is created
        and behaves. configuration settings include - EC2 instance, networking, storage etc..
     --saved configuration : Elastic beanstalk allows you to save environment configuration in the form of a template, it is refered to as configuration templates.
        this saved configuration can be applied to any existing environment to update its configuration. 
    --Platform: platforms in the elastic beanstalk is the entire collection of operating system, programming language runtime web and application servers.
      Beanstalk offers a variety of platforms for various application types.
          
           
       
     
    
    
     
            
     
        
        
   
      
    
        
      
      
     
   
     
         
   
    
     
    
 
 
              
      
          
 
      


    
    
    
    
    
    
    
      
    
          
      
     
     
     

      
    
    
        
   
          
  
    
  
   
